{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IndicTrans Demo Notebook",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## How to use the model with the API?"
      ],
      "metadata": {
        "id": "ObABytyveHl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json"
      ],
      "metadata": {
        "id": "4CsusQwV3uDc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# supported Indian languages\n",
        "\n",
        "# Assamese - as\n",
        "# Bengali - bn\n",
        "# Gujarati - gu\n",
        "# Hindi - hi\n",
        "# Kannada - kn\n",
        "# Malayalam - ml\n",
        "# Marathi - mr\n",
        "# Odia - or\n",
        "# Punjabi - pa\n",
        "# Tamil - ta\n",
        "# Telugu - te\n",
        "\n",
        "uri = \"http://216.48.181.177:5050\""
      ],
      "metadata": {
        "id": "jKkbgLhf3TSJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to translate single sentence, we use the translate_sentence endpoint\n",
        "API_URL = f\"{uri}/translate_sentence\"\n",
        "\n",
        "\n",
        "# In the JSON field of the request, we specify the text to translate, the source and target language\n",
        "response = requests.post(\n",
        "    API_URL,\n",
        "    json={\n",
        "  \"text\": \"The goal of AI4Bharat is to build language technologies for all Indian languages​\",\n",
        "  \"source_language\": \"en\",\n",
        "  \"target_language\": \"hi\"\n",
        "},\n",
        ")"
      ],
      "metadata": {
        "id": "bDDUxeYY3U0o"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = json.loads(response.text)\n",
        "print(f\"Request completed in {output['duration']} seconds and the translation is {output['text']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tdng_r43XUF",
        "outputId": "d1055ace-00f9-41d8-ca3e-d1ec40b044a2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Request completed in 0.81 seconds and the translation is एआई4भारत का लक्ष्य सभी भारतीय भाषाओं के लिए भाषा प्रौद्योगिकियों का निर्माण करना है\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore more on ULCA - https://bhashini.gov.in/ulca/model/explore-models"
      ],
      "metadata": {
        "id": "8EnSonxoeq7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Change to Hindi Examples"
      ],
      "metadata": {
        "id": "YNGuabA0ehY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to translate a batch of sentences, we use batch_translate endpoint\n",
        "API_URL = f\"{uri}/batch_translate\"\n",
        "\n",
        "# This is a sample batch of 4 tamil sentences. IF you have a large batch of sentences, please break it into smaller batches (typically of size 8 or 16) and query the API multiple times.\n",
        "\n",
        "sentence_batch = ['இத்தொற்றுநோய் உலகளாவிய சமூக மற்றும் பொருளாதார சீர்குலைவை ஏற்படுத்தியுள்ளது.',\n",
        " 'இதனால் பெரும் பொருளாதார மந்தநிலைக்குப் பின்னர் உலகளவில் மிகப்பெரிய மந்தநிலை ஏற்பட்டுள்ளது.',\n",
        " 'இது விளையாட்டு,மத, அரசியல் மற்றும் கலாச்சார நிகழ்வுகளை ஒத்திவைக்க அல்லது ரத்து செய்ய வழிவகுத்தது.',\n",
        " 'அச்சம் காரணமாக முகக்கவசம், கிருமிநாசினி உள்ளிட்ட பொருட்களை அதிக நபர்கள் வாங்கியதால் விநியோகப் பற்றாக்குறை ஏற்பட்டது.']\n",
        "\n",
        "# here we give the sentence_batch to \"text_lines\" and change the source and target language accordingly\n",
        "response = requests.post(\n",
        "    API_URL,\n",
        "    json={\n",
        "  \"text_lines\": sentence_batch,\n",
        "  \"source_language\": \"ta\",\n",
        "  \"target_language\": \"en\"\n",
        "},\n",
        ")\n",
        "\n",
        "output = json.loads(response.text)"
      ],
      "metadata": {
        "id": "bAHA_-bu3ZsR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for src, tgt in zip(sentence_batch, output['text_lines']):\n",
        "  print(f'src: {src} ----> tgt: {tgt}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93u5LQ0E3fGH",
        "outputId": "0b3d140c-9897-4c25-a8df-b7fa51c8b880"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src: இத்தொற்றுநோய் உலகளாவிய சமூக மற்றும் பொருளாதார சீர்குலைவை ஏற்படுத்தியுள்ளது. ----> tgt: The pandemic has caused global social and economic disruption.\n",
            "src: இதனால் பெரும் பொருளாதார மந்தநிலைக்குப் பின்னர் உலகளவில் மிகப்பெரிய மந்தநிலை ஏற்பட்டுள்ளது. ----> tgt: This has led to the worlds largest recession since the Great Depression.\n",
            "src: இது விளையாட்டு,மத, அரசியல் மற்றும் கலாச்சார நிகழ்வுகளை ஒத்திவைக்க அல்லது ரத்து செய்ய வழிவகுத்தது. ----> tgt: This led to the postponement or cancellation of sporting, religious, political and cultural events.\n",
            "src: அச்சம் காரணமாக முகக்கவசம், கிருமிநாசினி உள்ளிட்ட பொருட்களை அதிக நபர்கள் வாங்கியதால் விநியோகப் பற்றாக்குறை ஏற்பட்டது. ----> tgt: Due to this fear, there was a shortage of supply as most of the people purchased the items like masks, sanitizers etc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLI - Command Line Inference"
      ],
      "metadata": {
        "id": "FIhiob0HhiDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run on GPU runtime"
      ],
      "metadata": {
        "id": "HHj8u5sTth2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWXwlOeI64k4",
        "outputId": "9ca92690-294a-4116-f3c9-b0b60e282419"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jul 27 10:32:36 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a seperate folder to store everything\n",
        "!mkdir inference\n",
        "%cd inference"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zz9IXVqF6umc",
        "outputId": "c3b23102-761c-47bb-de44-d7e3cba1486b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/inference\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clone the repo for running evaluation\n",
        "!git clone https://github.com/AI4Bharat/indicTrans.git\n",
        "%cd indicTrans\n",
        "# clone requirements repositories\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!git clone https://github.com/rsennrich/subword-nmt.git\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTZTxNDihhH4",
        "outputId": "317bb946-6ad1-4680-a9b5-42a552da6313"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'indicTrans'...\n",
            "remote: Enumerating objects: 650, done.\u001b[K\n",
            "remote: Counting objects: 100% (353/353), done.\u001b[K\n",
            "remote: Compressing objects: 100% (164/164), done.\u001b[K\n",
            "remote: Total 650 (delta 248), reused 256 (delta 187), pack-reused 297\u001b[K\n",
            "Receiving objects: 100% (650/650), 1.71 MiB | 19.26 MiB/s, done.\n",
            "Resolving deltas: 100% (375/375), done.\n",
            "/content/inference/indicTrans\n",
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 1325, done.\u001b[K\n",
            "remote: Counting objects: 100% (107/107), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 1325 (delta 91), reused 82 (delta 82), pack-reused 1218\u001b[K\n",
            "Receiving objects: 100% (1325/1325), 9.55 MiB | 7.19 MiB/s, done.\n",
            "Resolving deltas: 100% (701/701), done.\n",
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 139 (delta 2), reused 2 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (139/139), 149.77 MiB | 17.77 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "Cloning into 'subword-nmt'...\n",
            "remote: Enumerating objects: 590, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 590 (delta 3), reused 4 (delta 1), pack-reused 576\u001b[K\n",
            "Receiving objects: 100% (590/590), 245.03 KiB | 272.00 KiB/s, done.\n",
            "Resolving deltas: 100% (352/352), done.\n",
            "/content/inference\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary libraries\n",
        "!pip install sacremoses pandas mock sacrebleu tensorboardX pyarrow indic-nlp-library\n",
        "# Install fairseq from source\n",
        "!git clone https://github.com/pytorch/fairseq.git\n",
        "%cd fairseq\n",
        "# !git checkout da9eaba12d82b9bfc1442f0e2c6fc1b895f4d35d\n",
        "!pip install ./\n",
        "! pip install xformers\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ET5pTbXc5D9V",
        "outputId": "3f2247bc-a313-4e2c-da5f-4b76d4c51071"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 31.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Collecting mock\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n",
            "\u001b[K     |████████████████████████████████| 116 kB 62.3 MB/s \n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 70.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (6.0.1)\n",
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.81-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2022.6.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.64.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (4.2.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.10)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Collecting morfessor\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Collecting sphinx-argparse\n",
            "  Downloading sphinx_argparse-0.3.1-py2.py3-none-any.whl (12 kB)\n",
            "Collecting sphinx-rtd-theme\n",
            "  Downloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 27.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx-argparse->indic-nlp-library) (1.8.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (57.4.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.23.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (21.3)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.12)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.10.3)\n",
            "Requirement already satisfied: docutils<0.18,>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.17.1)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.2.4)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.11.3)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.10)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.0.9)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.5)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=7da39a752afee199d68eceeeebe9747df052e1708858ba68448798c4fad734ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sphinx-rtd-theme, sphinx-argparse, portalocker, morfessor, colorama, tensorboardX, sacremoses, sacrebleu, mock, indic-nlp-library\n",
            "Successfully installed colorama-0.4.5 indic-nlp-library-0.81 mock-4.0.3 morfessor-2.0.6 portalocker-2.5.1 sacrebleu-2.2.0 sacremoses-0.0.53 sphinx-argparse-0.3.1 sphinx-rtd-theme-1.0.0 tensorboardX-2.5.1\n",
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 32094, done.\u001b[K\n",
            "remote: Counting objects: 100% (205/205), done.\u001b[K\n",
            "remote: Compressing objects: 100% (121/121), done.\u001b[K\n",
            "remote: Total 32094 (delta 104), reused 130 (delta 68), pack-reused 31889\u001b[K\n",
            "Receiving objects: 100% (32094/32094), 22.35 MiB | 17.23 MiB/s, done.\n",
            "Resolving deltas: 100% (23487/23487), done.\n",
            "/content/inference/fairseq\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/inference/fairseq\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bitarray\n",
            "  Downloading bitarray-2.6.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 21.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (0.29.30)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (1.12.0+cu113)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (0.12.0+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (4.64.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (2022.6.2)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (1.15.1)\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (1.21.6)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (2.2.0)\n",
            "Collecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 65.9 MB/s \n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 70.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2) (5.8.0)\n",
            "Collecting PyYAML>=5.1.*\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 61.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.1.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.2.6)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.4.5)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (2.5.1)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.8.10)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==0.12.2) (2.21)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core<1.1,>=1.0.7->fairseq==0.12.2) (3.8.1)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp37-cp37m-linux_x86_64.whl size=15417131 sha256=26327c534c3cdcb0c256d6c2eb6c830bc4d15f52e8d74153c45682444585637f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-q6c8rmgn/wheels/97/1c/f9/5124869884b553fc9e51cc6d0fe773091dc0cc5ac5f4942ece\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=42eda34d1763c48c72ab8d0be3b1b270b880c2a4c0f8e0d9d11bbf2b2ac58cdc\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: PyYAML, omegaconf, antlr4-python3-runtime, hydra-core, bitarray, fairseq\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-6.0 antlr4-python3-runtime-4.8 bitarray-2.6.0 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.11.tar.gz (219 kB)\n",
            "\u001b[K     |████████████████████████████████| 219 kB 30.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from xformers) (1.12.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xformers) (1.21.6)\n",
            "Collecting pyre-extensions==0.0.23\n",
            "  Downloading pyre_extensions-0.0.23-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pyre-extensions==0.0.23->xformers) (4.1.1)\n",
            "Collecting typing-inspect\n",
            "  Downloading typing_inspect-0.7.1-py3-none-any.whl (8.4 kB)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Building wheels for collected packages: xformers\n",
            "  Building wheel for xformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for xformers: filename=xformers-0.0.11-cp37-cp37m-linux_x86_64.whl size=6112559 sha256=d0f93cf349aad842747ac88f200a54f776dd5ab80577ba689579813267598b0e\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/68/bc/dfcdbce20dbf08da5f0ae62a75f270f20b2be0c8bb0f8c1587\n",
            "Successfully built xformers\n",
            "Installing collected packages: mypy-extensions, typing-inspect, pyre-extensions, xformers\n",
            "Successfully installed mypy-extensions-0.4.3 pyre-extensions-0.0.23 typing-inspect-0.7.1 xformers-0.0.11\n",
            "/content/inference\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add fairseq folder to python path\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "# sanity check to see if fairseq is installed\n",
        "from fairseq import checkpoint_utils, distributed_utils, options, tasks, utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fG-cAabKhwSP",
        "outputId": "67ae4744-4b1c-45ad-ca6d-bc222900efec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-07-27 10:39:03 | WARNING | root | Triton is not available, some optimizations will not be enabled.\n",
            "Error No module named 'triton'\n",
            "2022-07-27 10:39:03 | WARNING | root | Triton is not available, some optimizations will not be enabled.\n",
            "Error No module named 'triton'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the indictrans model\n",
        "\n",
        "\n",
        "# downloading the indic-en model\n",
        "!wget https://storage.googleapis.com/samanantar-public/V0.3/models/indic-en.zip\n",
        "\n",
        "# downloading the en-indic model\n",
        "!wget https://storage.googleapis.com/samanantar-public/V0.3/models/en-indic.zip\n",
        "\n",
        "# downloading the indic-indic model\n",
        "# !wget https://storage.googleapis.com/samanantar-public/V0.3/models/m2m.zip\n",
        "\n",
        "\n",
        "!unzip indic-en.zip\n",
        "!unzip en-indic.zip\n",
        "# !unzip m2m.zip\n",
        "\n",
        "!rm indic-en.zip\n",
        "!rm en-indic.zip\n",
        "# !rm m2m.zip\n",
        "\n",
        "%cd indicTrans/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oVF6GoHiIxx",
        "outputId": "e0b097b9-4998-4c2e-ff08-abb887388da9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-27 10:39:04--  https://storage.googleapis.com/samanantar-public/V0.3/models/indic-en.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.24.128, 142.250.4.128, 172.217.194.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.24.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4759117228 (4.4G) [application/zip]\n",
            "Saving to: ‘indic-en.zip’\n",
            "\n",
            "indic-en.zip        100%[===================>]   4.43G  69.0MB/s    in 65s     \n",
            "\n",
            "2022-07-27 10:40:09 (69.6 MB/s) - ‘indic-en.zip’ saved [4759117228/4759117228]\n",
            "\n",
            "Archive:  indic-en.zip\n",
            "   creating: indic-en/\n",
            "   creating: indic-en/vocab/\n",
            "  inflating: indic-en/vocab/bpe_codes.32k.SRC  \n",
            "  inflating: indic-en/vocab/vocab.SRC  \n",
            "  inflating: indic-en/vocab/vocab.TGT  \n",
            "  inflating: indic-en/vocab/bpe_codes.32k.TGT  \n",
            "   creating: indic-en/final_bin/\n",
            "  inflating: indic-en/final_bin/preprocess.log  \n",
            "  inflating: indic-en/final_bin/dict.TGT.txt  \n",
            "  inflating: indic-en/final_bin/test.SRC-TGT.SRC.idx  \n",
            "  inflating: indic-en/final_bin/test.SRC-TGT.TGT.idx  \n",
            "  inflating: indic-en/final_bin/train.SRC-TGT.SRC.idx  \n",
            "  inflating: indic-en/final_bin/dict.SRC.txt  \n",
            "  inflating: indic-en/final_bin/valid.SRC-TGT.TGT.idx  \n",
            "  inflating: indic-en/final_bin/test.SRC-TGT.TGT.bin  \n",
            "  inflating: indic-en/final_bin/valid.SRC-TGT.TGT.bin  \n",
            "  inflating: indic-en/final_bin/train.SRC-TGT.TGT.idx  \n",
            "  inflating: indic-en/final_bin/train.SRC-TGT.TGT.bin  \n",
            "  inflating: indic-en/final_bin/valid.SRC-TGT.SRC.idx  \n",
            "  inflating: indic-en/final_bin/train.SRC-TGT.SRC.bin  \n",
            "  inflating: indic-en/final_bin/valid.SRC-TGT.SRC.bin  \n",
            "  inflating: indic-en/final_bin/test.SRC-TGT.SRC.bin  \n",
            "   creating: indic-en/model/\n",
            "  inflating: indic-en/model/checkpoint_best.pt  \n",
            "unzip:  cannot find or open en-indic.zip, en-indic.zip.zip or en-indic.zip.ZIP.\n",
            "rm: cannot remove 'en-indic.zip': No such file or directory\n",
            "/content/inference/indicTrans\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a text file and adding en sentences we can use for testing the model\n",
        "!touch en_sentences.txt\n",
        "!echo 'This bicycle is too small for you !!' >> en_sentences.txt\n",
        "!echo \"I will directly meet you at the airport.\" >> en_sentences.txt\n",
        "!echo 'If COVID-19 is spreading in your community, stay safe by taking some simple precautions, such as physical distancing, wearing a mask, keeping rooms well ventilated, avoiding crowds, cleaning your hands, and coughing into a bent elbow or tissue' >> en_sentences.txt"
      ],
      "metadata": {
        "id": "xSPgedtsi17z"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# joint_translate takes source file, output file name, source language, target language, model directory as arguments\n",
        "!bash joint_translate.sh en_sentences.txt outputs.txt 'en' 'te' '../en-indic'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C8eg38MjlJ-",
        "outputId": "d139b308-89cb-48ce-cc0a-8364374c98b1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jul 27 10:41:20 UTC 2022\n",
            "Applying normalization and script conversion\n",
            "100% 3/3 [00:00<00:00, 110.49it/s]\n",
            "Number of sentences in input: 3\n",
            "Applying BPE\n",
            "usage: apply_bpe.py [-h] [--input PATH] --codes PATH [--merges INT]\n",
            "                    [--output PATH] [--separator STR] [--vocabulary PATH]\n",
            "                    [--vocabulary-threshold INT] [--dropout P]\n",
            "                    [--glossaries STR [STR ...]] [--seed S]\n",
            "                    [--num-workers NUM_WORKERS]\n",
            "apply_bpe.py: error: argument --codes/-c: can't open '../en-indic/vocab/bpe_codes.32k.SRC': [Errno 2] No such file or directory: '../en-indic/vocab/bpe_codes.32k.SRC'\n",
            "Decoding\n",
            "Extracting translations, script conversion and detokenization\n",
            "Translation completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat outputs.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ToyDW0hkXOe",
        "outputId": "d203d0fd-6c33-4cca-9247-9bbb0721f7ba"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!touch lang_sentences.txt\n",
        "!echo 'तुम आज सुबह यहाँ क्यों आए?' >> lang_sentences.txt\n",
        "!echo \"मेरे परिवार में हर कोई जल्दी उठता है।\" >> lang_sentences.txt\n",
        "!echo ' स्वास्थ्य और परिवार कल्याण मंत्रालय द्वारा प्रदान की गई जानकारी और सलाह को सावधानी व सही तरीके से पालन कर वायरस के स्थानीय प्रसार को रोका जा सकता है।' >> lang_sentences.txt"
      ],
      "metadata": {
        "id": "7xm7Pw-tkkqN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash joint_translate.sh lang_sentences.txt en_outputs.txt 'hi' 'en' '../indic-en'"
      ],
      "metadata": {
        "id": "OkJLBG_NlT5x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b16aa0f6-86f0-46dd-cc95-d233695535ca"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jul 27 10:41:44 UTC 2022\n",
            "Applying normalization and script conversion\n",
            "100% 3/3 [00:00<00:00, 106.39it/s]\n",
            "Number of sentences in input: 3\n",
            "Applying BPE\n",
            "Decoding\n",
            "Extracting translations, script conversion and detokenization\n",
            "Translation completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cat en_outputs.txt"
      ],
      "metadata": {
        "id": "4xn2EGeylXQO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43d1363e-8a20-4cf6-d459-7a763fdadf1a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did you come here this morning?\n",
            "Everyone in my family wakes up early.\n",
            "Local transmission of the virus can be prevented by following the information and advice given by the Ministry of Health and Family Welfare in a careful and correct manner.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRaNQMuDB1Sp",
        "outputId": "0612e00d-38c6-4650-c3b4-9b4936308f20"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/inference\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to Train the model?"
      ],
      "metadata": {
        "id": "zSa2X3UYlywz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1uWBFVHkI3QHGByLH_6bkLspANfO7CJ_n' height=300 width=300>"
      ],
      "metadata": {
        "id": "HzrufRqUL4M6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  lets now download the indictrans data dataset\n",
        "! wget https://storage.googleapis.com/samanantar-public/V0.3/sample_samanantar_v0.3.zip\n",
        "\n",
        "# full data - https://storage.googleapis.com/samanantar-public/V0.3/source_wise_splits.zip\n",
        "\n",
        "\n",
        "# lets also download the benchmarks for dev and test set\n",
        "! wget https://storage.googleapis.com/samanantar-public/benchmarks.zip\n",
        "\n",
        "!unzip sample_samanantar_v0.3.zip\n",
        "!unzip benchmarks.zip"
      ],
      "metadata": {
        "id": "gFMgdagvl1tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "wget --trust-server-names https://tinyurl.com/flores200dataset\n",
        "tar -xf flores200_dataset.tar.gz\n",
        "\n",
        "mkdir indic-en-exp\n",
        "# copying all the train folders to exp_dir\n",
        "cp -r sample_samanantar_v0.3/* indic-en-exp\n",
        "\n",
        "mkdir -p indic-en-exp/devtest/all\n",
        "\n",
        "for lang in as bn gu hi kn ml mr or pa ta te\n",
        "do\n",
        "mkdir -p indic-en-exp/devtest/all/en-$lang\n",
        "done\n",
        "\n",
        "cp flores200_dataset/dev/asm_Beng.dev indic-en-exp/devtest/all/en-as/dev.as\n",
        "cp flores200_dataset/dev/eng_Latn.dev indic-en-exp/devtest/all/en-as/dev.en\n",
        "\n",
        "cp flores200_dataset/dev/ben_Beng.dev indic-en-exp/devtest/all/en-bn/dev.bn\n",
        "cp flores200_dataset/dev/eng_Latn.dev indic-en-exp/devtest/all/en-bn/dev.en\n",
        "\n",
        "cp flores200_dataset/dev/guj_Gujr.dev indic-en-exp/devtest/all/en-gu/dev.gu\n",
        "cp flores200_dataset/dev/eng_Latn.dev indic-en-exp/devtest/all/en-gu/dev.en\n",
        "\n",
        "cp flores200_dataset/dev/hin_Deva.dev indic-en-exp/devtest/all/en-hi/dev.hi\n",
        "cp flores200_dataset/dev/eng_Latn.dev indic-en-exp/devtest/all/en-hi/dev.en\n",
        "\n",
        "cp flores200_dataset/dev/kan_Knda.dev indic-en-exp/devtest/all/en-kn/dev.kn\n",
        "cp flores200_dataset/dev/eng_Latn.dev indic-en-exp/devtest/all/en-kn/dev.en\n",
        "\n",
        "cp flores200_dataset/dev/mal_Mlym.dev indic-en-exp/devtest/all/en-ml/dev.ml\n",
        "cp flores200_dataset/dev/eng_Latn.dev indic-en-exp/devtest/all/en-ml/dev.en\n",
        "\n",
        "cp flores200_dataset/dev/mar_Deva.dev indic-en-exp/devtest/all/en-mr/dev.mr\n",
        "cp flores200_dataset/dev/eng_Latn.dev indic-en-exp/devtest/all/en-mr/dev.en\n",
        "\n",
        "cp flores200_dataset/dev/ory_Orya.dev indic-en-exp/devtest/all/en-or/dev.or\n",
        "cp flores200_dataset/dev/eng_Latn.dev indic-en-exp/devtest/all/en-or/dev.en\n",
        "\n",
        "cp flores200_dataset/dev/pan_Guru.dev indic-en-exp/devtest/all/en-pa/dev.pa\n",
        "cp flores200_dataset/dev/eng_Latn.dev indic-en-exp/devtest/all/en-pa/dev.en\n",
        "\n",
        "cp flores200_dataset/dev/tam_Taml.dev indic-en-exp/devtest/all/en-ta/dev.ta\n",
        "cp flores200_dataset/dev/eng_Latn.dev indic-en-exp/devtest/all/en-ta/dev.en\n",
        "\n",
        "cp flores200_dataset/dev/tel_Telu.dev indic-en-exp/devtest/all/en-te/dev.te\n",
        "cp flores200_dataset/dev/eng_Latn.dev indic-en-exp/devtest/all/en-te/dev.en\n",
        "\n",
        "cp flores200_dataset/devtest/asm_Beng.devtest indic-en-exp/devtest/all/en-as/test.as\n",
        "cp flores200_dataset/devtest/eng_Latn.devtest indic-en-exp/devtest/all/en-as/test.en\n",
        "\n",
        "cp flores200_dataset/devtest/ben_Beng.devtest indic-en-exp/devtest/all/en-bn/test.bn\n",
        "cp flores200_dataset/devtest/eng_Latn.devtest indic-en-exp/devtest/all/en-bn/test.en\n",
        "\n",
        "cp flores200_dataset/devtest/guj_Gujr.devtest indic-en-exp/devtest/all/en-gu/test.gu\n",
        "cp flores200_dataset/devtest/eng_Latn.devtest indic-en-exp/devtest/all/en-gu/test.en\n",
        "\n",
        "cp flores200_dataset/devtest/hin_Deva.devtest indic-en-exp/devtest/all/en-hi/test.hi\n",
        "cp flores200_dataset/devtest/eng_Latn.devtest indic-en-exp/devtest/all/en-hi/test.en\n",
        "\n",
        "cp flores200_dataset/devtest/kan_Knda.devtest indic-en-exp/devtest/all/en-kn/test.kn\n",
        "cp flores200_dataset/devtest/eng_Latn.devtest indic-en-exp/devtest/all/en-kn/test.en\n",
        "\n",
        "cp flores200_dataset/devtest/mal_Mlym.devtest indic-en-exp/devtest/all/en-ml/test.ml\n",
        "cp flores200_dataset/devtest/eng_Latn.devtest indic-en-exp/devtest/all/en-ml/test.en\n",
        "\n",
        "cp flores200_dataset/devtest/mar_Deva.devtest indic-en-exp/devtest/all/en-mr/test.mr\n",
        "cp flores200_dataset/devtest/eng_Latn.devtest indic-en-exp/devtest/all/en-mr/test.en\n",
        "\n",
        "cp flores200_dataset/devtest/ory_Orya.devtest indic-en-exp/devtest/all/en-or/test.or\n",
        "cp flores200_dataset/devtest/eng_Latn.devtest indic-en-exp/devtest/all/en-or/test.en\n",
        "\n",
        "cp flores200_dataset/devtest/pan_Guru.devtest indic-en-exp/devtest/all/en-pa/test.pa\n",
        "cp flores200_dataset/devtest/eng_Latn.devtest indic-en-exp/devtest/all/en-pa/test.en\n",
        "\n",
        "cp flores200_dataset/devtest/tam_Taml.devtest indic-en-exp/devtest/all/en-ta/test.ta\n",
        "cp flores200_dataset/devtest/eng_Latn.devtest indic-en-exp/devtest/all/en-ta/test.en\n",
        "\n",
        "cp flores200_dataset/devtest/tel_Telu.devtest indic-en-exp/devtest/all/en-te/test.te\n",
        "cp flores200_dataset/devtest/eng_Latn.devtest indic-en-exp/devtest/all/en-te/test.en'\n",
        "\n"
      ],
      "metadata": {
        "id": "kcw6ySLWFL41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd indicTrans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyEeDAN2QRsy",
        "outputId": "23e4bcd3-e375-4bca-fd9a-9b8ab366dcfc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/inference/indicTrans\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare_data_joint_training.sh takes experiment dir, src_lang, tgt_lang as input \n",
        "# This does preprocessing, building vocab, binarization for joint training\n",
        "\n",
        "# The learning  and applying vocabulary will take a while if the dataset is huge. To make it faster, run it on a multicore system\n",
        "\n",
        "! bash prepare_data_joint_training.sh '../indic-en-exp' 'indic' 'en'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqGybxM8CcMS",
        "outputId": "3836b8f3-d967-42e6-83e9-4207eb919f4b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiment ../indic-en-exp on indic to en\n",
            "Applying normalization and script conversion for train\n",
            "100% 1000/1000 [00:00<00:00, 8144.85it/s]\n",
            "100% 1000/1000 [00:00<00:00, 1992.56it/s]\n",
            "Number of sentences in train: 1000\n",
            "Applying normalization and script conversion for dev\n",
            "100% 997/997 [00:00<00:00, 7115.61it/s]\n",
            "100% 997/997 [00:00<00:00, 2039.18it/s]\n",
            "Number of sentences in dev: 997\n",
            "Applying normalization and script conversion for test\n",
            "100% 1012/1012 [00:00<00:00, 5973.46it/s]\n",
            "100% 1012/1012 [00:00<00:00, 1954.44it/s]\n",
            "Number of sentences in test: 1012\n",
            "Applying normalization and script conversion for train\n",
            "100% 1000/1000 [00:00<00:00, 11588.84it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2206.67it/s]\n",
            "Number of sentences in train: 1000\n",
            "Applying normalization and script conversion for dev\n",
            "100% 997/997 [00:00<00:00, 7105.96it/s]\n",
            "100% 997/997 [00:00<00:00, 1978.27it/s]\n",
            "Number of sentences in dev: 997\n",
            "Applying normalization and script conversion for test\n",
            "100% 1012/1012 [00:00<00:00, 6369.11it/s]\n",
            "100% 1012/1012 [00:00<00:00, 2038.53it/s]\n",
            "Number of sentences in test: 1012\n",
            "Applying normalization and script conversion for train\n",
            "100% 1000/1000 [00:00<00:00, 7758.48it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2090.69it/s]\n",
            "Number of sentences in train: 1000\n",
            "Applying normalization and script conversion for dev\n",
            "100% 997/997 [00:00<00:00, 6804.57it/s]\n",
            "100% 997/997 [00:00<00:00, 2043.85it/s]\n",
            "Number of sentences in dev: 997\n",
            "Applying normalization and script conversion for test\n",
            "100% 1012/1012 [00:00<00:00, 6830.36it/s]\n",
            "100% 1012/1012 [00:00<00:00, 2003.71it/s]\n",
            "Number of sentences in test: 1012\n",
            "Applying normalization and script conversion for train\n",
            "100% 1000/1000 [00:00<00:00, 9160.49it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2284.52it/s]\n",
            "Number of sentences in train: 1000\n",
            "Applying normalization and script conversion for dev\n",
            "100% 997/997 [00:00<00:00, 6500.43it/s]\n",
            "100% 997/997 [00:00<00:00, 2057.50it/s]\n",
            "Number of sentences in dev: 997\n",
            "Applying normalization and script conversion for test\n",
            "100% 1012/1012 [00:00<00:00, 7132.83it/s]\n",
            "100% 1012/1012 [00:00<00:00, 2017.45it/s]\n",
            "Number of sentences in test: 1012\n",
            "Applying normalization and script conversion for train\n",
            "100% 1000/1000 [00:00<00:00, 9033.61it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2225.03it/s]\n",
            "Number of sentences in train: 1000\n",
            "Applying normalization and script conversion for dev\n",
            "100% 997/997 [00:00<00:00, 6208.43it/s]\n",
            "100% 997/997 [00:00<00:00, 1978.57it/s]\n",
            "Number of sentences in dev: 997\n",
            "Applying normalization and script conversion for test\n",
            "100% 1012/1012 [00:00<00:00, 5605.89it/s]\n",
            "100% 1012/1012 [00:00<00:00, 1955.88it/s]\n",
            "Number of sentences in test: 1012\n",
            "Applying normalization and script conversion for train\n",
            "100% 1000/1000 [00:00<00:00, 10886.80it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2093.00it/s]\n",
            "Number of sentences in train: 1000\n",
            "Applying normalization and script conversion for dev\n",
            "100% 997/997 [00:00<00:00, 6301.31it/s]\n",
            "100% 997/997 [00:00<00:00, 1936.67it/s]\n",
            "Number of sentences in dev: 997\n",
            "Applying normalization and script conversion for test\n",
            "100% 1012/1012 [00:00<00:00, 6746.04it/s]\n",
            "100% 1012/1012 [00:00<00:00, 1999.28it/s]\n",
            "Number of sentences in test: 1012\n",
            "Applying normalization and script conversion for train\n",
            "100% 1000/1000 [00:00<00:00, 8440.43it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2076.60it/s]\n",
            "Number of sentences in train: 1000\n",
            "Applying normalization and script conversion for dev\n",
            "100% 997/997 [00:00<00:00, 6940.75it/s]\n",
            "100% 997/997 [00:00<00:00, 1962.34it/s]\n",
            "Number of sentences in dev: 997\n",
            "Applying normalization and script conversion for test\n",
            "100% 1012/1012 [00:00<00:00, 6797.67it/s]\n",
            "100% 1012/1012 [00:00<00:00, 1960.04it/s]\n",
            "Number of sentences in test: 1012\n",
            "Applying normalization and script conversion for train\n",
            "100% 1000/1000 [00:00<00:00, 8107.18it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2294.92it/s]\n",
            "Number of sentences in train: 1000\n",
            "Applying normalization and script conversion for dev\n",
            "100% 997/997 [00:00<00:00, 6830.98it/s]\n",
            "100% 997/997 [00:00<00:00, 1967.41it/s]\n",
            "Number of sentences in dev: 997\n",
            "Applying normalization and script conversion for test\n",
            "100% 1012/1012 [00:00<00:00, 5760.56it/s]\n",
            "100% 1012/1012 [00:00<00:00, 1911.59it/s]\n",
            "Number of sentences in test: 1012\n",
            "Applying normalization and script conversion for train\n",
            "100% 1000/1000 [00:00<00:00, 7422.34it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2074.23it/s]\n",
            "Number of sentences in train: 1000\n",
            "Applying normalization and script conversion for dev\n",
            "100% 997/997 [00:00<00:00, 5822.71it/s]\n",
            "100% 997/997 [00:00<00:00, 1998.88it/s]\n",
            "Number of sentences in dev: 997\n",
            "Applying normalization and script conversion for test\n",
            "100% 1012/1012 [00:00<00:00, 6429.75it/s]\n",
            "100% 1012/1012 [00:00<00:00, 2013.02it/s]\n",
            "Number of sentences in test: 1012\n",
            "Applying normalization and script conversion for train\n",
            "100% 1000/1000 [00:00<00:00, 8111.22it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2106.90it/s]\n",
            "Number of sentences in train: 1000\n",
            "Applying normalization and script conversion for dev\n",
            "100% 997/997 [00:00<00:00, 6298.91it/s]\n",
            "100% 997/997 [00:00<00:00, 1982.33it/s]\n",
            "Number of sentences in dev: 997\n",
            "Applying normalization and script conversion for test\n",
            "100% 1012/1012 [00:00<00:00, 6391.29it/s]\n",
            "100% 1012/1012 [00:00<00:00, 1958.16it/s]\n",
            "Number of sentences in test: 1012\n",
            "Applying normalization and script conversion for train\n",
            "100% 1000/1000 [00:00<00:00, 9347.83it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2228.52it/s]\n",
            "Number of sentences in train: 1000\n",
            "Applying normalization and script conversion for dev\n",
            "100% 997/997 [00:00<00:00, 7191.46it/s]\n",
            "100% 997/997 [00:00<00:00, 1974.92it/s]\n",
            "Number of sentences in dev: 997\n",
            "Applying normalization and script conversion for test\n",
            "100% 1012/1012 [00:00<00:00, 6415.69it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"scripts/preprocess_translate.py\", line 172, in <module>\n",
            "    print(preprocess(infname, outfname, lang, transliterate))\n",
            "  File \"scripts/preprocess_translate.py\", line 61, in preprocess\n",
            "    num_lines = sum(1 for line in open(infname, \"r\"))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '../indic-en-exp/devtest/all/en-te/test.en'\n",
            "Number of sentences in test: \n",
            "\n",
            "../indic-en-exp/data/train.SRC\n",
            "../indic-en-exp/data/train.TGT\n",
            "  0% 0/11 [00:00<?, ?it/s]src: as, tgt:en\n",
            "../indic-en-exp/norm/as-en/train.as\n",
            "../indic-en-exp/norm/as-en/train.en\n",
            "src: bn, tgt:en\n",
            "../indic-en-exp/norm/bn-en/train.bn\n",
            "../indic-en-exp/norm/bn-en/train.en\n",
            "src: gu, tgt:en\n",
            "../indic-en-exp/norm/gu-en/train.gu\n",
            "../indic-en-exp/norm/gu-en/train.en\n",
            "src: hi, tgt:en\n",
            "../indic-en-exp/norm/hi-en/train.hi\n",
            "../indic-en-exp/norm/hi-en/train.en\n",
            "src: kn, tgt:en\n",
            "../indic-en-exp/norm/kn-en/train.kn\n",
            "../indic-en-exp/norm/kn-en/train.en\n",
            "src: ml, tgt:en\n",
            "../indic-en-exp/norm/ml-en/train.ml\n",
            "../indic-en-exp/norm/ml-en/train.en\n",
            "src: mr, tgt:en\n",
            "../indic-en-exp/norm/mr-en/train.mr\n",
            "../indic-en-exp/norm/mr-en/train.en\n",
            "src: or, tgt:en\n",
            "../indic-en-exp/norm/or-en/train.or\n",
            "../indic-en-exp/norm/or-en/train.en\n",
            "src: pa, tgt:en\n",
            "../indic-en-exp/norm/pa-en/train.pa\n",
            "../indic-en-exp/norm/pa-en/train.en\n",
            "src: ta, tgt:en\n",
            "../indic-en-exp/norm/ta-en/train.ta\n",
            "../indic-en-exp/norm/ta-en/train.en\n",
            "src: te, tgt:en\n",
            "../indic-en-exp/norm/te-en/train.te\n",
            "../indic-en-exp/norm/te-en/train.en\n",
            "100% 11/11 [00:00<00:00, 99.18it/s]\n",
            "  0% 0/11 [00:00<?, ?it/s]src: as, tgt:en\n",
            "../indic-en-exp/norm/as-en/train.as\n",
            "src: bn, tgt:en\n",
            "../indic-en-exp/norm/bn-en/train.bn\n",
            "src: gu, tgt:en\n",
            "../indic-en-exp/norm/gu-en/train.gu\n",
            "src: hi, tgt:en\n",
            "../indic-en-exp/norm/hi-en/train.hi\n",
            "src: kn, tgt:en\n",
            "../indic-en-exp/norm/kn-en/train.kn\n",
            "src: ml, tgt:en\n",
            "../indic-en-exp/norm/ml-en/train.ml\n",
            "src: mr, tgt:en\n",
            "../indic-en-exp/norm/mr-en/train.mr\n",
            "src: or, tgt:en\n",
            "../indic-en-exp/norm/or-en/train.or\n",
            "src: pa, tgt:en\n",
            "../indic-en-exp/norm/pa-en/train.pa\n",
            "src: ta, tgt:en\n",
            "../indic-en-exp/norm/ta-en/train.ta\n",
            "src: te, tgt:en\n",
            "../indic-en-exp/norm/te-en/train.te\n",
            "100% 11/11 [00:00<00:00, 1235.34it/s]\n",
            "\n",
            "../indic-en-exp/data/dev.SRC\n",
            "../indic-en-exp/data/dev.TGT\n",
            "  0% 0/11 [00:00<?, ?it/s]src: as, tgt:en\n",
            "../indic-en-exp/norm/as-en/dev.as\n",
            "../indic-en-exp/norm/as-en/dev.en\n",
            "src: bn, tgt:en\n",
            "../indic-en-exp/norm/bn-en/dev.bn\n",
            "../indic-en-exp/norm/bn-en/dev.en\n",
            "src: gu, tgt:en\n",
            "../indic-en-exp/norm/gu-en/dev.gu\n",
            "../indic-en-exp/norm/gu-en/dev.en\n",
            "src: hi, tgt:en\n",
            "../indic-en-exp/norm/hi-en/dev.hi\n",
            "../indic-en-exp/norm/hi-en/dev.en\n",
            "src: kn, tgt:en\n",
            "../indic-en-exp/norm/kn-en/dev.kn\n",
            "../indic-en-exp/norm/kn-en/dev.en\n",
            "src: ml, tgt:en\n",
            "../indic-en-exp/norm/ml-en/dev.ml\n",
            "../indic-en-exp/norm/ml-en/dev.en\n",
            "src: mr, tgt:en\n",
            "../indic-en-exp/norm/mr-en/dev.mr\n",
            "../indic-en-exp/norm/mr-en/dev.en\n",
            "src: or, tgt:en\n",
            "../indic-en-exp/norm/or-en/dev.or\n",
            "../indic-en-exp/norm/or-en/dev.en\n",
            "src: pa, tgt:en\n",
            "../indic-en-exp/norm/pa-en/dev.pa\n",
            "../indic-en-exp/norm/pa-en/dev.en\n",
            "src: ta, tgt:en\n",
            "../indic-en-exp/norm/ta-en/dev.ta\n",
            "../indic-en-exp/norm/ta-en/dev.en\n",
            "src: te, tgt:en\n",
            "../indic-en-exp/norm/te-en/dev.te\n",
            "../indic-en-exp/norm/te-en/dev.en\n",
            "100% 11/11 [00:00<00:00, 109.03it/s]\n",
            "  0% 0/11 [00:00<?, ?it/s]src: as, tgt:en\n",
            "../indic-en-exp/norm/as-en/dev.as\n",
            "src: bn, tgt:en\n",
            "../indic-en-exp/norm/bn-en/dev.bn\n",
            "src: gu, tgt:en\n",
            "../indic-en-exp/norm/gu-en/dev.gu\n",
            "src: hi, tgt:en\n",
            "../indic-en-exp/norm/hi-en/dev.hi\n",
            "src: kn, tgt:en\n",
            "../indic-en-exp/norm/kn-en/dev.kn\n",
            "src: ml, tgt:en\n",
            "../indic-en-exp/norm/ml-en/dev.ml\n",
            "src: mr, tgt:en\n",
            "../indic-en-exp/norm/mr-en/dev.mr\n",
            "src: or, tgt:en\n",
            "../indic-en-exp/norm/or-en/dev.or\n",
            "src: pa, tgt:en\n",
            "../indic-en-exp/norm/pa-en/dev.pa\n",
            "src: ta, tgt:en\n",
            "../indic-en-exp/norm/ta-en/dev.ta\n",
            "src: te, tgt:en\n",
            "../indic-en-exp/norm/te-en/dev.te\n",
            "100% 11/11 [00:00<00:00, 785.50it/s]\n",
            "\n",
            "../indic-en-exp/data/test.SRC\n",
            "../indic-en-exp/data/test.TGT\n",
            "  0% 0/11 [00:00<?, ?it/s]src: as, tgt:en\n",
            "../indic-en-exp/norm/as-en/test.as\n",
            "../indic-en-exp/norm/as-en/test.en\n",
            "src: bn, tgt:en\n",
            "../indic-en-exp/norm/bn-en/test.bn\n",
            "../indic-en-exp/norm/bn-en/test.en\n",
            "src: gu, tgt:en\n",
            "../indic-en-exp/norm/gu-en/test.gu\n",
            "../indic-en-exp/norm/gu-en/test.en\n",
            "src: hi, tgt:en\n",
            "../indic-en-exp/norm/hi-en/test.hi\n",
            "../indic-en-exp/norm/hi-en/test.en\n",
            "src: kn, tgt:en\n",
            "../indic-en-exp/norm/kn-en/test.kn\n",
            "../indic-en-exp/norm/kn-en/test.en\n",
            "src: ml, tgt:en\n",
            "../indic-en-exp/norm/ml-en/test.ml\n",
            "../indic-en-exp/norm/ml-en/test.en\n",
            "src: mr, tgt:en\n",
            "../indic-en-exp/norm/mr-en/test.mr\n",
            "../indic-en-exp/norm/mr-en/test.en\n",
            "src: or, tgt:en\n",
            "../indic-en-exp/norm/or-en/test.or\n",
            "../indic-en-exp/norm/or-en/test.en\n",
            "src: pa, tgt:en\n",
            "../indic-en-exp/norm/pa-en/test.pa\n",
            "../indic-en-exp/norm/pa-en/test.en\n",
            "src: ta, tgt:en\n",
            "../indic-en-exp/norm/ta-en/test.ta\n",
            "../indic-en-exp/norm/ta-en/test.en\n",
            "src: te, tgt:en\n",
            "100% 11/11 [00:00<00:00, 121.34it/s]\n",
            "  0% 0/11 [00:00<?, ?it/s]src: as, tgt:en\n",
            "../indic-en-exp/norm/as-en/test.as\n",
            "src: bn, tgt:en\n",
            "../indic-en-exp/norm/bn-en/test.bn\n",
            "src: gu, tgt:en\n",
            "../indic-en-exp/norm/gu-en/test.gu\n",
            "src: hi, tgt:en\n",
            "../indic-en-exp/norm/hi-en/test.hi\n",
            "src: kn, tgt:en\n",
            "../indic-en-exp/norm/kn-en/test.kn\n",
            "src: ml, tgt:en\n",
            "../indic-en-exp/norm/ml-en/test.ml\n",
            "src: mr, tgt:en\n",
            "../indic-en-exp/norm/mr-en/test.mr\n",
            "src: or, tgt:en\n",
            "../indic-en-exp/norm/or-en/test.or\n",
            "src: pa, tgt:en\n",
            "../indic-en-exp/norm/pa-en/test.pa\n",
            "src: ta, tgt:en\n",
            "../indic-en-exp/norm/ta-en/test.ta\n",
            "src: te, tgt:en\n",
            "../indic-en-exp/norm/te-en/test.te\n",
            "100% 11/11 [00:00<00:00, 763.22it/s]\n",
            "Wed Jul 27 10:44:54 UTC 2022\n",
            "Input file: ../indic-en-exp/data/train\n",
            "learning source BPE\n",
            " 88% 28051/32000 [01:00<00:03, 1309.72it/s]no pair has frequency >= 2. Stopping\n",
            " 88% 28281/32000 [01:01<00:08, 460.84it/s] \n",
            "learning target BPE\n",
            " 48% 15247/32000 [00:15<00:07, 2124.85it/s]no pair has frequency >= 2. Stopping\n",
            " 49% 15530/32000 [00:15<00:16, 996.98it/s] \n",
            "computing SRC vocab\n",
            "computing TGT vocab\n",
            "Wed Jul 27 10:46:16 UTC 2022\n",
            "train\n",
            "Apply to SRC corpus\n",
            "subword-nmt/subword_nmt/apply_bpe.py:444: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "Apply to TGT corpus\n",
            "subword-nmt/subword_nmt/apply_bpe.py:444: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "dev\n",
            "Apply to SRC corpus\n",
            "subword-nmt/subword_nmt/apply_bpe.py:444: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "Apply to TGT corpus\n",
            "subword-nmt/subword_nmt/apply_bpe.py:444: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "test\n",
            "Apply to SRC corpus\n",
            "subword-nmt/subword_nmt/apply_bpe.py:444: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "Apply to TGT corpus\n",
            "subword-nmt/subword_nmt/apply_bpe.py:444: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "Adding language tags\n",
            "11000it [00:00, 231150.18it/s]\n",
            "10967it [00:00, 111298.06it/s]\n",
            "10120it [00:00, 160590.65it/s]\n",
            "2022-07-27 10:46:27 | WARNING | root | Triton is not available, some optimizations will not be enabled.\n",
            "Error No module named 'triton'\n",
            "2022-07-27 10:46:27 | WARNING | root | Triton is not available, some optimizations will not be enabled.\n",
            "Error No module named 'triton'\n",
            "2022-07-27 10:46:28 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='../indic-en-exp/final_bin', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='SRC', srcdict=None, suppress_crashes=False, target_lang='TGT', task='translation', tensorboard_logdir=None, testpref='../indic-en-exp/final/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=5, thresholdtgt=5, tokenizer=None, tpu=False, trainpref='../indic-en-exp/final/train', use_plasma_view=False, user_dir=None, validpref='../indic-en-exp/final/dev', wandb_project=None, workers=2)\n",
            "2022-07-27 10:46:30 | INFO | fairseq_cli.preprocess | [SRC] Dictionary: 7224 types\n",
            "2022-07-27 10:46:32 | INFO | fairseq_cli.preprocess | [SRC] ../indic-en-exp/final/train.SRC: 11000 sents, 285865 tokens, 0.063% replaced (by <unk>)\n",
            "2022-07-27 10:46:32 | INFO | fairseq_cli.preprocess | [SRC] Dictionary: 7224 types\n",
            "2022-07-27 10:46:35 | INFO | fairseq_cli.preprocess | [SRC] ../indic-en-exp/final/dev.SRC: 10967 sents, 543443 tokens, 0.0662% replaced (by <unk>)\n",
            "2022-07-27 10:46:35 | INFO | fairseq_cli.preprocess | [SRC] Dictionary: 7224 types\n",
            "2022-07-27 10:46:38 | INFO | fairseq_cli.preprocess | [SRC] ../indic-en-exp/final/test.SRC: 10120 sents, 514678 tokens, 0.0847% replaced (by <unk>)\n",
            "2022-07-27 10:46:38 | INFO | fairseq_cli.preprocess | [TGT] Dictionary: 4632 types\n",
            "2022-07-27 10:46:39 | INFO | fairseq_cli.preprocess | [TGT] ../indic-en-exp/final/train.TGT: 11000 sents, 214933 tokens, 0.00791% replaced (by <unk>)\n",
            "2022-07-27 10:46:39 | INFO | fairseq_cli.preprocess | [TGT] Dictionary: 4632 types\n",
            "2022-07-27 10:46:41 | INFO | fairseq_cli.preprocess | [TGT] ../indic-en-exp/final/dev.TGT: 10967 sents, 420673 tokens, 0.0967% replaced (by <unk>)\n",
            "2022-07-27 10:46:41 | INFO | fairseq_cli.preprocess | [TGT] Dictionary: 4632 types\n",
            "2022-07-27 10:46:43 | INFO | fairseq_cli.preprocess | [TGT] ../indic-en-exp/final/test.TGT: 10120 sents, 404910 tokens, 0.138% replaced (by <unk>)\n",
            "2022-07-27 10:46:43 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to ../indic-en-exp/final_bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!(fairseq-train ../indic-en-exp/final_bin \\\n",
        "--max-source-positions=210 \\\n",
        "--max-target-positions=210 \\\n",
        "--max-update=1000 \\\n",
        "--save-interval=1 \\\n",
        "--arch=transformer_4x \\\n",
        "--criterion=label_smoothed_cross_entropy \\\n",
        "--source-lang=SRC \\\n",
        "--lr-scheduler=inverse_sqrt \\\n",
        "--target-lang=TGT \\\n",
        "--label-smoothing=0.1 \\\n",
        "--optimizer adam \\\n",
        "--adam-betas \"(0.9, 0.98)\" \\\n",
        "--clip-norm 1.0 \\\n",
        "--warmup-init-lr 1e-07 \\\n",
        "--lr 0.0005 \\\n",
        "--warmup-updates 4000 \\\n",
        "--dropout 0.2 \\\n",
        "--save-dir ../indic-en-exp/model \\\n",
        "--keep-last-epochs 5 \\\n",
        "--patience 5 \\\n",
        "--skip-invalid-size-inputs-valid-test \\\n",
        "--fp16 \\\n",
        "--user-dir model_configs \\\n",
        "--update-freq=1 \\\n",
        "--distributed-world-size 1 \\\n",
        "--max-tokens 1024)\n",
        "\n",
        "# Important Arguments\n",
        "# --max-updates         -> maximum update steps the model will be trained for\n",
        "# --arch=transformer_4x -> we use a custom transformer model and name it transformer_4x (4 times the parameter size of transformer  base)\n",
        "# --user_dir            -> we define the custom transformer arch in model_configs folder and pass it as an argument to user_dir for fairseq to register this architechture\n",
        "# --lr                  -> learning rate. From our limited experiments, we find that lower learning rates like 3e-5 works best for finetuning.\n",
        "# --max_tokens          -> this is max tokens per batch. You should limit to lower values if you get oom errors.\n",
        "# --update-freq         -> gradient accumulation steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZMMQ68TCdjb",
        "outputId": "354cf705-6998-4b0c-d1b9-2f22bebbe996"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-07-27 10:48:00 | WARNING | root | Triton is not available, some optimizations will not be enabled.\n",
            "Error No module named 'triton'\n",
            "2022-07-27 10:48:00 | WARNING | root | Triton is not available, some optimizations will not be enabled.\n",
            "Error No module named 'triton'\n",
            "2022-07-27 10:48:02 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': 'model_configs', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 1024, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1024, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 1000, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '../indic-en-exp/model', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 5, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 5, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_4x', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer_4x', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='../indic-en-exp/final_bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=16, decoder_embed_dim=1536, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1536, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=1536, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.2, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1536, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=5, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=210, max_target_positions=210, max_tokens=1024, max_tokens_valid=1024, max_update=1000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_src_tgt_embed=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=5, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='../indic-en-exp/model', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang='SRC', stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang='TGT', task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='model_configs', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': '../indic-en-exp/final_bin', 'source_lang': 'SRC', 'target_lang': 'TGT', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 210, 'max_target_positions': 210, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2022-07-27 10:48:02 | INFO | fairseq.tasks.translation | [SRC] dictionary: 7224 types\n",
            "2022-07-27 10:48:02 | INFO | fairseq.tasks.translation | [TGT] dictionary: 4632 types\n",
            "2022-07-27 10:48:06 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(7224, 1536, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1536, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1536, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1536, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1536, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1536, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1536, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(4632, 1536, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1536, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1536, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1536, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1536, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1536, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1536, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=1536, out_features=4632, bias=False)\n",
            "  )\n",
            ")\n",
            "2022-07-27 10:48:06 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2022-07-27 10:48:06 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2022-07-27 10:48:06 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2022-07-27 10:48:06 | INFO | fairseq_cli.train | num. shared model params: 478,651,392 (num. trained: 478,651,392)\n",
            "2022-07-27 10:48:06 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2022-07-27 10:48:06 | INFO | fairseq.data.data_utils | loaded 10,967 examples from: ../indic-en-exp/final_bin/valid.SRC-TGT.SRC\n",
            "2022-07-27 10:48:06 | INFO | fairseq.data.data_utils | loaded 10,967 examples from: ../indic-en-exp/final_bin/valid.SRC-TGT.TGT\n",
            "2022-07-27 10:48:06 | INFO | fairseq.tasks.translation | ../indic-en-exp/final_bin valid SRC-TGT 10967 examples\n",
            "2022-07-27 10:48:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-07-27 10:48:09 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \n",
            "2022-07-27 10:48:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-07-27 10:48:09 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2022-07-27 10:48:09 | INFO | fairseq_cli.train | max tokens per device = 1024 and max sentences per device = None\n",
            "2022-07-27 10:48:09 | INFO | fairseq.trainer | Preparing to load checkpoint ../indic-en-exp/model/checkpoint_last.pt\n",
            "2022-07-27 10:48:09 | INFO | fairseq.trainer | No existing checkpoint found ../indic-en-exp/model/checkpoint_last.pt\n",
            "2022-07-27 10:48:09 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2022-07-27 10:48:09 | INFO | fairseq.data.data_utils | loaded 11,000 examples from: ../indic-en-exp/final_bin/train.SRC-TGT.SRC\n",
            "2022-07-27 10:48:09 | INFO | fairseq.data.data_utils | loaded 11,000 examples from: ../indic-en-exp/final_bin/train.SRC-TGT.TGT\n",
            "2022-07-27 10:48:09 | INFO | fairseq.tasks.translation | ../indic-en-exp/final_bin train SRC-TGT 11000 examples\n",
            "2022-07-27 10:48:09 | WARNING | fairseq.tasks.fairseq_task | 7 samples have invalid sizes and will be skipped, max_positions=(210, 210), first few sample ids=[9827, 8266, 8273, 3089, 8918, 1539, 3399]\n",
            "2022-07-27 10:48:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 389\n",
            "epoch 001:   0% 0/389 [00:00<?, ?it/s]2022-07-27 10:48:09 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2022-07-27 10:48:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 001: 100% 388/389 [01:52<00:00,  3.53it/s, loss=10.008, nll_loss=9.611, ppl=782.12, wps=1888.1, ups=3.47, wpb=544.5, bsz=25.6, num_updates=300, lr=3.75925e-05, gnorm=3.877, clip=100, loss_scale=128, train_wall=28, gb_free=4.3, wall=87]2022-07-27 10:50:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/719 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   0% 1/719 [00:00<01:35,  7.54it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   1% 4/719 [00:00<00:38, 18.77it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   1% 7/719 [00:00<00:31, 22.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   1% 10/719 [00:00<00:29, 24.24it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   2% 13/719 [00:00<00:28, 24.64it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   2% 16/719 [00:00<00:27, 25.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   3% 19/719 [00:00<00:26, 26.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   3% 22/719 [00:00<00:26, 26.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   3% 25/719 [00:01<00:26, 26.68it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   4% 28/719 [00:01<00:26, 26.09it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   4% 31/719 [00:01<00:26, 25.73it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   5% 34/719 [00:01<00:26, 25.92it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   5% 37/719 [00:01<00:26, 26.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   6% 40/719 [00:01<00:25, 26.78it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   6% 43/719 [00:01<00:26, 25.92it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   6% 46/719 [00:01<00:25, 26.36it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   7% 49/719 [00:01<00:28, 23.79it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   7% 52/719 [00:02<00:26, 25.03it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   8% 55/719 [00:02<00:26, 24.76it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   8% 58/719 [00:02<00:25, 25.67it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   8% 61/719 [00:02<00:24, 26.73it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   9% 64/719 [00:02<00:24, 26.38it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   9% 67/719 [00:02<00:24, 26.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  10% 70/719 [00:02<00:24, 26.48it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  10% 73/719 [00:02<00:24, 26.81it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  11% 76/719 [00:02<00:23, 27.38it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  11% 79/719 [00:03<00:24, 26.38it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  11% 82/719 [00:03<00:23, 27.24it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  12% 85/719 [00:03<00:22, 27.81it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  12% 88/719 [00:03<00:22, 27.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  13% 91/719 [00:03<00:23, 27.10it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  13% 94/719 [00:03<00:22, 27.46it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  13% 97/719 [00:03<00:22, 27.57it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  14% 100/719 [00:03<00:23, 26.75it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  14% 103/719 [00:03<00:23, 26.53it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  15% 106/719 [00:04<00:22, 27.42it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  15% 109/719 [00:04<00:21, 27.84it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  16% 112/719 [00:04<00:22, 27.26it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  16% 115/719 [00:04<00:21, 27.58it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  16% 118/719 [00:04<00:21, 27.58it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  17% 121/719 [00:04<00:22, 26.86it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  17% 124/719 [00:04<00:23, 25.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  18% 127/719 [00:04<00:22, 26.54it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  18% 130/719 [00:04<00:21, 26.92it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  18% 133/719 [00:05<00:21, 27.17it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  19% 136/719 [00:05<00:22, 26.49it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  19% 139/719 [00:05<00:21, 26.60it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  20% 142/719 [00:05<00:21, 26.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  20% 145/719 [00:05<00:22, 25.92it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  21% 148/719 [00:05<00:22, 25.15it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  21% 152/719 [00:05<00:21, 26.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  22% 155/719 [00:05<00:21, 26.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  22% 158/719 [00:06<00:21, 26.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  22% 161/719 [00:06<00:21, 26.19it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  23% 165/719 [00:06<00:19, 27.82it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  24% 169/719 [00:06<00:18, 30.07it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  24% 173/719 [00:06<00:17, 30.75it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  25% 177/719 [00:06<00:17, 30.79it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  25% 181/719 [00:06<00:17, 31.06it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  26% 185/719 [00:06<00:16, 32.38it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  26% 189/719 [00:06<00:15, 33.19it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  27% 193/719 [00:07<00:15, 33.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  27% 197/719 [00:07<00:15, 32.78it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  28% 201/719 [00:07<00:16, 31.73it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  29% 205/719 [00:07<00:15, 32.43it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  29% 209/719 [00:07<00:15, 32.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  30% 213/719 [00:07<00:15, 32.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  30% 217/719 [00:07<00:15, 31.70it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  31% 221/719 [00:07<00:15, 32.70it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  31% 225/719 [00:08<00:14, 32.98it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  32% 229/719 [00:08<00:14, 33.54it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  32% 233/719 [00:08<00:14, 32.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  33% 237/719 [00:08<00:14, 32.38it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  34% 241/719 [00:08<00:14, 33.03it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  34% 245/719 [00:08<00:14, 32.54it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  35% 249/719 [00:08<00:14, 32.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  35% 253/719 [00:08<00:14, 31.46it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  36% 257/719 [00:09<00:14, 32.27it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  36% 261/719 [00:09<00:13, 32.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  37% 265/719 [00:09<00:13, 32.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  37% 269/719 [00:09<00:13, 32.26it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  38% 273/719 [00:09<00:14, 30.61it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  39% 277/719 [00:09<00:14, 30.57it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  39% 281/719 [00:09<00:14, 29.44it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  39% 284/719 [00:09<00:15, 28.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  40% 287/719 [00:10<00:15, 28.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  40% 290/719 [00:10<00:14, 28.82it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  41% 294/719 [00:10<00:14, 29.87it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  41% 298/719 [00:10<00:14, 30.03it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  42% 301/719 [00:10<00:13, 29.88it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  42% 304/719 [00:10<00:13, 29.68it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  43% 307/719 [00:10<00:14, 28.29it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  43% 310/719 [00:10<00:14, 28.52it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  44% 313/719 [00:10<00:14, 28.48it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  44% 316/719 [00:11<00:14, 28.28it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  44% 319/719 [00:11<00:14, 27.84it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  45% 322/719 [00:11<00:14, 27.53it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  45% 325/719 [00:11<00:14, 27.52it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  46% 329/719 [00:11<00:13, 28.50it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  46% 332/719 [00:11<00:13, 28.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  47% 335/719 [00:11<00:13, 28.58it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  47% 338/719 [00:11<00:13, 28.49it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  47% 341/719 [00:11<00:13, 27.69it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  48% 344/719 [00:12<00:13, 27.84it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  48% 347/719 [00:12<00:13, 27.80it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  49% 350/719 [00:12<00:13, 27.86it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  49% 353/719 [00:12<00:13, 26.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  50% 356/719 [00:12<00:13, 26.88it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  50% 359/719 [00:12<00:13, 27.65it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  50% 362/719 [00:12<00:12, 27.58it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  51% 365/719 [00:12<00:12, 27.88it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  51% 368/719 [00:12<00:12, 27.01it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  52% 371/719 [00:13<00:13, 26.57it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  52% 374/719 [00:13<00:12, 26.89it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  52% 377/719 [00:13<00:12, 26.77it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  53% 380/719 [00:13<00:12, 26.37it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  53% 383/719 [00:13<00:12, 26.64it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  54% 386/719 [00:13<00:12, 27.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  54% 389/719 [00:13<00:11, 27.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  55% 392/719 [00:13<00:11, 27.84it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  55% 395/719 [00:13<00:12, 26.89it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  55% 398/719 [00:14<00:12, 26.57it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  56% 401/719 [00:14<00:11, 26.60it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  56% 404/719 [00:14<00:12, 26.06it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  57% 407/719 [00:14<00:12, 25.84it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  57% 410/719 [00:14<00:11, 26.60it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  57% 413/719 [00:14<00:11, 26.57it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  58% 416/719 [00:14<00:11, 26.75it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  58% 419/719 [00:14<00:11, 26.88it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  59% 422/719 [00:15<00:10, 27.01it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  59% 425/719 [00:15<00:11, 26.70it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  60% 428/719 [00:15<00:11, 25.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  60% 431/719 [00:15<00:10, 26.85it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  60% 434/719 [00:15<00:10, 27.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  61% 437/719 [00:15<00:10, 27.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  61% 440/719 [00:15<00:10, 26.80it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  62% 444/719 [00:15<00:09, 27.79it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  62% 447/719 [00:15<00:09, 27.51it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  63% 450/719 [00:16<00:10, 26.47it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  63% 453/719 [00:16<00:10, 25.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  64% 457/719 [00:16<00:09, 27.27it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  64% 460/719 [00:16<00:09, 27.48it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  64% 463/719 [00:16<00:09, 27.02it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  65% 466/719 [00:16<00:09, 26.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  65% 469/719 [00:16<00:09, 27.04it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  66% 472/719 [00:16<00:09, 25.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  66% 475/719 [00:17<00:09, 24.85it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  66% 478/719 [00:17<00:09, 25.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  67% 481/719 [00:17<00:09, 25.38it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  67% 484/719 [00:17<00:09, 25.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  68% 487/719 [00:17<00:08, 25.93it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  68% 491/719 [00:17<00:08, 27.19it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  69% 494/719 [00:17<00:08, 27.71it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  69% 498/719 [00:17<00:07, 28.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  70% 501/719 [00:17<00:07, 28.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  70% 505/719 [00:18<00:07, 29.25it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  71% 508/719 [00:18<00:08, 23.73it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  71% 511/719 [00:18<00:08, 25.03it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  72% 515/719 [00:18<00:07, 26.58it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  72% 519/719 [00:18<00:07, 28.01it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  73% 523/719 [00:18<00:06, 29.10it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  73% 526/719 [00:18<00:07, 24.83it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  74% 529/719 [00:19<00:07, 25.40it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  74% 533/719 [00:19<00:06, 27.13it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  75% 536/719 [00:19<00:06, 27.79it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  75% 540/719 [00:19<00:05, 30.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  76% 544/719 [00:19<00:05, 32.55it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  76% 548/719 [00:19<00:05, 33.58it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  77% 552/719 [00:19<00:04, 34.79it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  77% 556/719 [00:19<00:04, 35.47it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  78% 560/719 [00:19<00:04, 35.93it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  78% 564/719 [00:20<00:04, 37.03it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  79% 568/719 [00:20<00:04, 36.92it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  80% 572/719 [00:20<00:03, 36.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  80% 576/719 [00:20<00:03, 36.81it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  81% 580/719 [00:20<00:03, 36.11it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  81% 585/719 [00:20<00:03, 37.49it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  82% 589/719 [00:20<00:03, 37.17it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  82% 593/719 [00:20<00:03, 36.41it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  83% 597/719 [00:20<00:03, 36.17it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  84% 601/719 [00:21<00:03, 35.49it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  84% 605/719 [00:21<00:03, 36.56it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  85% 609/719 [00:21<00:03, 36.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  85% 613/719 [00:21<00:02, 35.67it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  86% 617/719 [00:21<00:02, 35.30it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  86% 621/719 [00:21<00:02, 35.54it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  87% 625/719 [00:21<00:02, 35.70it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  87% 629/719 [00:21<00:02, 34.67it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  88% 633/719 [00:21<00:02, 34.32it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  89% 637/719 [00:22<00:02, 33.90it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  89% 641/719 [00:22<00:02, 34.64it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  90% 645/719 [00:22<00:02, 34.01it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  90% 649/719 [00:22<00:02, 33.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  91% 653/719 [00:22<00:01, 34.44it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  91% 657/719 [00:22<00:01, 34.60it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  92% 661/719 [00:22<00:01, 34.02it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  92% 665/719 [00:22<00:01, 33.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  93% 669/719 [00:23<00:01, 32.90it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  94% 673/719 [00:23<00:01, 33.09it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  94% 677/719 [00:23<00:01, 32.78it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  95% 681/719 [00:23<00:01, 32.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  95% 685/719 [00:23<00:01, 31.77it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  96% 689/719 [00:23<00:00, 30.61it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  96% 693/719 [00:23<00:00, 30.70it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  97% 697/719 [00:23<00:00, 31.15it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  97% 701/719 [00:24<00:00, 30.11it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  98% 705/719 [00:24<00:00, 29.30it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  98% 708/719 [00:24<00:00, 27.85it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  99% 711/719 [00:24<00:00, 27.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  99% 714/719 [00:24<00:00, 26.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 717/719 [00:24<00:00, 25.72it/s]\u001b[A\n",
            "                                                                          \u001b[A2022-07-27 10:50:27 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.02 | nll_loss 9.621 | ppl 787.3 | wps 17061.4 | wpb 585.1 | bsz 15.3 | num_updates 389\n",
            "2022-07-27 10:50:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 389 updates\n",
            "2022-07-27 10:50:27 | INFO | fairseq.trainer | Saving checkpoint to /content/inference/indic-en-exp/model/checkpoint1.pt\n",
            "tcmalloc: large alloc 1914609664 bytes == 0xe8252000 @  0x7fa2c52a8b6b 0x7fa2c52c8379 0x7fa24e0f4d57 0x7fa24e0e2bc3 0x7fa277fd756f 0x7fa277fd77ba 0x7fa277fd781f 0x7fa2784ea059 0x7fa278d5a9c0 0x7fa278aa643e 0x7fa278d33c7e 0x7fa278ae62c2 0x7fa2784e4501 0x7fa278eabe2b 0x7fa2788311cf 0x7fa278d35dfc 0x7fa2788311cf 0x7fa279da755b 0x7fa279da79be 0x7fa2788a9164 0x7fa2784d9a1b 0x7fa278fa92e1 0x7fa2789d9109 0x7fa29fc8751d 0x7fa29fc887d6 0x59b1b0 0x515655 0x549576 0x593fce 0x5118f8 0x549e0e\n",
            "tcmalloc: large alloc 1914609664 bytes == 0x15ac3c000 @  0x7fa2c52a8b6b 0x7fa2c52c8379 0x7fa24e0f4d57 0x7fa24e0e2bc3 0x7fa277fd756f 0x7fa277fd77ba 0x7fa277fd781f 0x7fa2784ea059 0x7fa278d5a9c0 0x7fa278aa643e 0x7fa278d33c7e 0x7fa278ae62c2 0x7fa2784e4501 0x7fa278eabe2b 0x7fa2788311cf 0x7fa278d35dfc 0x7fa2788311cf 0x7fa279da755b 0x7fa279da79be 0x7fa2788a9164 0x7fa2784d9a1b 0x7fa278fa92e1 0x7fa2789d9109 0x7fa29fc8751d 0x7fa29fc887d6 0x59b1b0 0x515655 0x549576 0x593fce 0x5118f8 0x549e0e\n",
            "tcmalloc: large alloc 1914609664 bytes == 0x7f9e4f614000 @  0x7fa2c52c61e7 0x4a3940 0x5b438c 0x7fa29ff633f8 0x7fa279ca85d5 0x7fa279ca2b21 0x7fa279ca9c29 0x7fa29ff639fb 0x7fa29fbedd61 0x593784 0x594731 0x548cc1 0x51566f 0x549e0e 0x593fce 0x5118f8 0x549576 0x593fce 0x548ae9 0x51566f 0x593dd7 0x5118f8 0x549576 0x593fce 0x548ae9 0x5127f1 0x593dd7 0x511e2c 0x549e0e 0x593fce 0x548ae9\n",
            "tcmalloc: large alloc 1914609664 bytes == 0x7f9e4f614000 @  0x7fa2c52c61e7 0x4a3940 0x5b438c 0x7fa29ff633f8 0x7fa279ca85d5 0x7fa279ca2b21 0x7fa279ca9c29 0x7fa29ff639fb 0x7fa29fbedd61 0x593784 0x594731 0x548cc1 0x51566f 0x549e0e 0x593fce 0x5118f8 0x549576 0x593fce 0x548ae9 0x51566f 0x593dd7 0x5118f8 0x549576 0x593fce 0x548ae9 0x5127f1 0x593dd7 0x511e2c 0x549e0e 0x593fce 0x548ae9\n",
            "2022-07-27 10:50:56 | INFO | fairseq.trainer | Finished saving checkpoint to /content/inference/indic-en-exp/model/checkpoint1.pt\n",
            "2022-07-27 10:52:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../indic-en-exp/model/checkpoint1.pt (epoch 1 @ 389 updates, score 10.02) (writing took 112.10394780899992 seconds)\n",
            "2022-07-27 10:52:19 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2022-07-27 10:52:19 | INFO | train | epoch 001 | loss 10.301 | nll_loss 9.955 | ppl 992.41 | wps 855.4 | ups 1.56 | wpb 548 | bsz 28.3 | num_updates 389 | lr 4.87153e-05 | gnorm 4.841 | clip 100 | loss_scale 128 | train_wall 111 | gb_free 4.3 | wall 250\n",
            "2022-07-27 10:52:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 389\n",
            "epoch 002:   0% 0/389 [00:00<?, ?it/s]2022-07-27 10:52:19 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2022-07-27 10:52:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002: 100% 388/389 [01:52<00:00,  3.49it/s, loss=9.425, nll_loss=8.934, ppl=489.23, wps=1941.3, ups=3.43, wpb=566.6, bsz=29.8, num_updates=700, lr=8.75825e-05, gnorm=3.021, clip=100, loss_scale=128, train_wall=29, gb_free=4.3, wall=340]2022-07-27 10:54:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/719 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   0% 1/719 [00:00<01:24,  8.54it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   1% 4/719 [00:00<00:37, 19.18it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   1% 7/719 [00:00<00:31, 22.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   1% 10/719 [00:00<00:29, 23.65it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   2% 13/719 [00:00<00:28, 25.09it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   2% 16/719 [00:00<00:27, 26.01it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   3% 19/719 [00:00<00:26, 26.40it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   3% 22/719 [00:00<00:26, 25.91it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   3% 25/719 [00:01<00:27, 25.62it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   4% 28/719 [00:01<00:26, 26.07it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   4% 31/719 [00:01<00:26, 26.23it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   5% 34/719 [00:01<00:26, 25.91it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   5% 37/719 [00:01<00:26, 26.04it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   6% 40/719 [00:01<00:25, 26.77it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   6% 43/719 [00:01<00:25, 26.06it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   6% 46/719 [00:01<00:25, 26.69it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   7% 49/719 [00:01<00:26, 25.74it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   7% 52/719 [00:02<00:25, 26.29it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   8% 55/719 [00:02<00:24, 27.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   8% 58/719 [00:02<00:23, 27.55it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   9% 62/719 [00:02<00:22, 28.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   9% 66/719 [00:02<00:22, 29.11it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  10% 69/719 [00:02<00:22, 28.52it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  10% 72/719 [00:02<00:22, 28.15it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  10% 75/719 [00:02<00:23, 27.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  11% 78/719 [00:02<00:23, 27.43it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  11% 81/719 [00:03<00:23, 27.42it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  12% 84/719 [00:03<00:22, 27.97it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  12% 87/719 [00:03<00:22, 27.65it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  13% 90/719 [00:03<00:23, 27.08it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  13% 93/719 [00:03<00:22, 27.50it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  13% 96/719 [00:03<00:22, 27.35it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  14% 99/719 [00:03<00:23, 26.42it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  14% 102/719 [00:03<00:24, 25.44it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  15% 105/719 [00:03<00:23, 26.40it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  15% 108/719 [00:04<00:22, 26.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  15% 111/719 [00:04<00:22, 26.64it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  16% 114/719 [00:04<00:23, 26.23it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  16% 117/719 [00:04<00:23, 25.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  17% 120/719 [00:04<00:22, 26.17it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  17% 123/719 [00:04<00:23, 25.42it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  18% 126/719 [00:04<00:23, 25.66it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  18% 129/719 [00:04<00:22, 26.31it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  18% 132/719 [00:05<00:22, 26.55it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  19% 135/719 [00:05<00:22, 25.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  19% 138/719 [00:05<00:22, 25.49it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  20% 141/719 [00:05<00:22, 25.81it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  20% 144/719 [00:05<00:22, 25.59it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  20% 147/719 [00:05<00:22, 25.03it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  21% 150/719 [00:05<00:22, 25.69it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  21% 153/719 [00:05<00:21, 26.23it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  22% 156/719 [00:05<00:21, 25.88it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  22% 159/719 [00:06<00:22, 25.18it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  23% 162/719 [00:06<00:21, 25.79it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  23% 166/719 [00:06<00:19, 28.36it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  24% 170/719 [00:06<00:18, 29.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  24% 174/719 [00:06<00:17, 30.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  25% 178/719 [00:06<00:17, 30.24it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  25% 182/719 [00:06<00:17, 30.53it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  26% 186/719 [00:06<00:16, 31.89it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  26% 190/719 [00:07<00:16, 32.50it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  27% 194/719 [00:07<00:16, 32.72it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  28% 198/719 [00:07<00:16, 31.81it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  28% 202/719 [00:07<00:16, 31.80it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  29% 206/719 [00:07<00:16, 31.87it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  29% 210/719 [00:07<00:16, 31.78it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  30% 214/719 [00:07<00:16, 31.14it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  30% 218/719 [00:07<00:16, 30.77it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  31% 222/719 [00:08<00:15, 31.62it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  31% 226/719 [00:08<00:15, 31.66it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  32% 230/719 [00:08<00:15, 31.55it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  33% 234/719 [00:08<00:15, 31.65it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  33% 238/719 [00:08<00:15, 31.50it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  34% 242/719 [00:08<00:15, 31.79it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  34% 246/719 [00:08<00:15, 31.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  35% 250/719 [00:08<00:15, 31.19it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  35% 254/719 [00:09<00:15, 30.81it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  36% 258/719 [00:09<00:14, 31.63it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  36% 262/719 [00:09<00:14, 31.97it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  37% 266/719 [00:09<00:14, 31.79it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  38% 270/719 [00:09<00:14, 31.51it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  38% 274/719 [00:09<00:14, 30.14it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  39% 278/719 [00:09<00:14, 29.84it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  39% 281/719 [00:09<00:14, 29.26it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  39% 284/719 [00:10<00:14, 29.09it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  40% 287/719 [00:10<00:15, 28.16it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  40% 290/719 [00:10<00:15, 28.54it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  41% 294/719 [00:10<00:14, 29.50it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  41% 297/719 [00:10<00:14, 29.42it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  42% 300/719 [00:10<00:14, 29.43it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  42% 303/719 [00:10<00:14, 29.48it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  43% 306/719 [00:10<00:14, 28.76it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  43% 309/719 [00:10<00:14, 28.06it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  43% 312/719 [00:11<00:14, 28.43it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  44% 315/719 [00:11<00:14, 28.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  44% 318/719 [00:11<00:14, 27.81it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  45% 321/719 [00:11<00:14, 27.70it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  45% 324/719 [00:11<00:14, 27.19it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  45% 327/719 [00:11<00:14, 27.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  46% 330/719 [00:11<00:13, 28.43it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  46% 333/719 [00:11<00:13, 28.64it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  47% 336/719 [00:11<00:13, 28.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  47% 339/719 [00:12<00:13, 28.07it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  48% 342/719 [00:12<00:13, 27.51it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  48% 345/719 [00:12<00:13, 27.42it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  48% 348/719 [00:12<00:13, 27.55it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  49% 351/719 [00:12<00:13, 26.91it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  49% 354/719 [00:12<00:14, 25.80it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 357/719 [00:12<00:13, 26.80it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 360/719 [00:12<00:13, 26.99it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 363/719 [00:12<00:13, 27.18it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  51% 366/719 [00:13<00:13, 26.85it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  51% 369/719 [00:13<00:13, 26.27it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  52% 372/719 [00:13<00:13, 26.55it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  52% 375/719 [00:13<00:12, 26.56it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  53% 378/719 [00:13<00:12, 26.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  53% 381/719 [00:13<00:13, 25.53it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  53% 384/719 [00:13<00:12, 26.54it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  54% 387/719 [00:13<00:12, 27.05it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  54% 390/719 [00:13<00:12, 27.12it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  55% 393/719 [00:14<00:12, 26.73it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  55% 396/719 [00:14<00:12, 25.70it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  55% 399/719 [00:14<00:12, 25.56it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  56% 402/719 [00:14<00:12, 25.17it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  56% 405/719 [00:14<00:12, 24.55it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  57% 408/719 [00:14<00:12, 25.12it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  57% 411/719 [00:14<00:12, 25.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  58% 414/719 [00:14<00:11, 26.18it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  58% 417/719 [00:15<00:12, 24.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  58% 420/719 [00:15<00:11, 25.52it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  59% 423/719 [00:15<00:11, 25.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  59% 426/719 [00:15<00:11, 25.05it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  60% 429/719 [00:15<00:11, 24.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  60% 433/719 [00:15<00:10, 26.38it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  61% 436/719 [00:15<00:10, 26.58it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  61% 439/719 [00:15<00:10, 26.41it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  61% 442/719 [00:15<00:10, 26.27it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  62% 445/719 [00:16<00:10, 26.69it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  62% 448/719 [00:16<00:10, 25.97it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  63% 451/719 [00:16<00:10, 24.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  63% 454/719 [00:16<00:10, 25.35it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  64% 458/719 [00:16<00:09, 26.64it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  64% 461/719 [00:16<00:09, 26.75it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  65% 464/719 [00:16<00:09, 25.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  65% 467/719 [00:16<00:09, 26.83it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  65% 470/719 [00:17<00:09, 26.66it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  66% 473/719 [00:17<00:09, 25.51it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  66% 476/719 [00:17<00:09, 24.34it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  67% 479/719 [00:17<00:09, 24.85it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  67% 482/719 [00:17<00:09, 24.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  67% 485/719 [00:17<00:09, 25.74it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  68% 489/719 [00:17<00:07, 28.75it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  69% 493/719 [00:17<00:07, 30.91it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  69% 497/719 [00:17<00:06, 32.29it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  70% 501/719 [00:18<00:06, 33.41it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  70% 505/719 [00:18<00:06, 33.70it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  71% 509/719 [00:18<00:06, 34.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  71% 513/719 [00:18<00:05, 36.07it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  72% 517/719 [00:18<00:05, 36.02it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  72% 521/719 [00:18<00:05, 36.00it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  73% 525/719 [00:18<00:05, 36.29it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  74% 529/719 [00:18<00:05, 35.87it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  74% 533/719 [00:18<00:05, 35.57it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  75% 537/719 [00:19<00:04, 36.50it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  75% 541/719 [00:19<00:04, 36.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  76% 545/719 [00:19<00:04, 36.80it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  76% 549/719 [00:19<00:04, 36.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  77% 553/719 [00:19<00:04, 36.12it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  77% 557/719 [00:19<00:04, 35.60it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  78% 561/719 [00:19<00:04, 36.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  79% 565/719 [00:19<00:04, 36.25it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  79% 569/719 [00:19<00:04, 35.72it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  80% 573/719 [00:20<00:04, 35.60it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  80% 577/719 [00:20<00:04, 34.87it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  81% 581/719 [00:20<00:03, 34.64it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  81% 585/719 [00:20<00:03, 34.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  82% 589/719 [00:20<00:03, 35.23it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  82% 593/719 [00:20<00:03, 34.06it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  83% 597/719 [00:20<00:03, 34.28it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  84% 601/719 [00:20<00:03, 34.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  84% 605/719 [00:21<00:03, 34.28it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  85% 609/719 [00:21<00:03, 34.84it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  85% 613/719 [00:21<00:03, 34.47it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  86% 617/719 [00:21<00:03, 33.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  86% 621/719 [00:21<00:02, 34.72it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  87% 625/719 [00:21<00:02, 35.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  87% 629/719 [00:21<00:02, 34.01it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  88% 633/719 [00:21<00:02, 33.70it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  89% 637/719 [00:21<00:02, 32.87it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  89% 641/719 [00:22<00:02, 33.69it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  90% 645/719 [00:22<00:02, 33.56it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  90% 649/719 [00:22<00:02, 33.00it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  91% 653/719 [00:22<00:01, 33.75it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  91% 657/719 [00:22<00:01, 33.76it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  92% 661/719 [00:22<00:01, 33.17it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  92% 665/719 [00:22<00:01, 33.19it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  93% 669/719 [00:22<00:01, 32.42it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  94% 673/719 [00:23<00:01, 32.54it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  94% 677/719 [00:23<00:01, 32.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  95% 681/719 [00:23<00:01, 32.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  95% 685/719 [00:23<00:01, 30.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  96% 689/719 [00:23<00:01, 29.59it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  96% 693/719 [00:23<00:00, 29.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  97% 697/719 [00:23<00:00, 30.45it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  97% 701/719 [00:24<00:00, 29.04it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  98% 704/719 [00:24<00:00, 28.78it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  98% 707/719 [00:24<00:00, 27.31it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  99% 710/719 [00:24<00:00, 26.18it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  99% 713/719 [00:24<00:00, 26.01it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 716/719 [00:24<00:00, 24.67it/s]\u001b[A\n",
            "                                                                          \u001b[A2022-07-27 10:54:37 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.38 | nll_loss 8.862 | ppl 465.3 | wps 17076.8 | wpb 585.1 | bsz 15.3 | num_updates 778 | best_loss 9.38\n",
            "2022-07-27 10:54:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 778 updates\n",
            "2022-07-27 10:54:37 | INFO | fairseq.trainer | Saving checkpoint to /content/inference/indic-en-exp/model/checkpoint2.pt\n",
            "tcmalloc: large alloc 1914609664 bytes == 0x15ac3c000 @  0x7fa2c52a8b6b 0x7fa2c52c8379 0x7fa24e0f4d57 0x7fa24e0e2bc3 0x7fa277fd756f 0x7fa277fd77ba 0x7fa277fd781f 0x7fa2784ea059 0x7fa278d5a9c0 0x7fa278aa643e 0x7fa278d33c7e 0x7fa278ae62c2 0x7fa2784e4501 0x7fa278eabe2b 0x7fa2788311cf 0x7fa278d35dfc 0x7fa2788311cf 0x7fa279da755b 0x7fa279da79be 0x7fa2788a9164 0x7fa2784d9a1b 0x7fa278fa92e1 0x7fa2789d9109 0x7fa29fc8751d 0x7fa29fc887d6 0x59b1b0 0x515655 0x549576 0x593fce 0x5118f8 0x549e0e\n",
            "terminate called after throwing an instance of 'c10::Error'\n",
            "  what():  [enforce fail at inline_container.cc:319] . unexpected pos 1385868928 vs 1385868820\n",
            "frame #0: c10::ThrowEnforceNotMet(char const*, int, char const*, std::string const&, void const*) + 0x55 (0x7fa24e1040a5 in /usr/local/lib/python3.7/dist-packages/torch/lib/libc10.so)\n",
            "frame #1: <unknown function> + 0x343966c (0x7fa279ca866c in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #2: mz_zip_writer_add_mem_ex_v2 + 0x5b1 (0x7fa279ca2a41 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #3: caffe2::serialize::PyTorchStreamWriter::writeRecord(std::string const&, void const*, unsigned long, bool) + 0xb9 (0x7fa279ca9c29 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #4: caffe2::serialize::PyTorchStreamWriter::writeEndOfFile() + 0x2c3 (0x7fa279caa0f3 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #5: caffe2::serialize::PyTorchStreamWriter::~PyTorchStreamWriter() + 0x125 (0x7fa279caa365 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #6: <unknown function> + 0x709565 (0x7fa29ff7b565 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_python.so)\n",
            "frame #7: <unknown function> + 0x37a13f (0x7fa29fbec13f in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_python.so)\n",
            "frame #8: <unknown function> + 0x37b05f (0x7fa29fbed05f in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_python.so)\n",
            "frame #9: /usr/bin/python3() [0x5a9afc]\n",
            "frame #10: /usr/bin/python3() [0x4fa7d8]\n",
            "frame #11: /usr/bin/python3() [0x4fa7ec]\n",
            "frame #12: _PyEval_EvalFrameDefault + 0x2a33 (0x513f83 in /usr/bin/python3)\n",
            "frame #13: _PyFunction_FastCallKeywords + 0x187 (0x593dd7 in /usr/bin/python3)\n",
            "frame #14: _PyEval_EvalFrameDefault + 0x3a8 (0x5118f8 in /usr/bin/python3)\n",
            "frame #15: _PyEval_EvalCodeWithName + 0x346 (0x549576 in /usr/bin/python3)\n",
            "frame #16: _PyFunction_FastCallKeywords + 0x37e (0x593fce in /usr/bin/python3)\n",
            "frame #17: /usr/bin/python3() [0x548ae9]\n",
            "frame #18: _PyEval_EvalFrameDefault + 0x12a1 (0x5127f1 in /usr/bin/python3)\n",
            "frame #19: _PyFunction_FastCallKeywords + 0x187 (0x593dd7 in /usr/bin/python3)\n",
            "frame #20: _PyEval_EvalFrameDefault + 0x8dc (0x511e2c in /usr/bin/python3)\n",
            "frame #21: _PyEval_EvalCodeWithName + 0xbde (0x549e0e in /usr/bin/python3)\n",
            "frame #22: _PyFunction_FastCallKeywords + 0x37e (0x593fce in /usr/bin/python3)\n",
            "frame #23: /usr/bin/python3() [0x548ae9]\n",
            "frame #24: _PyEval_EvalFrameDefault + 0x411f (0x51566f in /usr/bin/python3)\n",
            "frame #25: _PyFunction_FastCallKeywords + 0x187 (0x593dd7 in /usr/bin/python3)\n",
            "frame #26: _PyEval_EvalFrameDefault + 0x3a8 (0x5118f8 in /usr/bin/python3)\n",
            "frame #27: _PyFunction_FastCallDict + 0x15a (0x4bc98a in /usr/bin/python3)\n",
            "frame #28: _PyEval_EvalFrameDefault + 0x1f56 (0x5134a6 in /usr/bin/python3)\n",
            "frame #29: _PyEval_EvalCodeWithName + 0xbde (0x549e0e in /usr/bin/python3)\n",
            "frame #30: _PyFunction_FastCallKeywords + 0x37e (0x593fce in /usr/bin/python3)\n",
            "frame #31: _PyEval_EvalFrameDefault + 0x3a8 (0x5118f8 in /usr/bin/python3)\n",
            "frame #32: _PyFunction_FastCallDict + 0x15a (0x4bc98a in /usr/bin/python3)\n",
            "frame #33: _PyEval_EvalFrameDefault + 0x1f56 (0x5134a6 in /usr/bin/python3)\n",
            "frame #34: _PyEval_EvalCodeWithName + 0x346 (0x549576 in /usr/bin/python3)\n",
            "frame #35: _PyFunction_FastCallKeywords + 0x37e (0x593fce in /usr/bin/python3)\n",
            "frame #36: /usr/bin/python3() [0x548ae9]\n",
            "frame #37: _PyEval_EvalFrameDefault + 0x411f (0x51566f in /usr/bin/python3)\n",
            "frame #38: /usr/bin/python3() [0x635afd]\n",
            "frame #39: _PyEval_EvalFrameDefault + 0x3a8 (0x5118f8 in /usr/bin/python3)\n",
            "frame #40: _PyEval_EvalCodeWithName + 0x346 (0x549576 in /usr/bin/python3)\n",
            "frame #41: PyEval_EvalCode + 0x23 (0x604173 in /usr/bin/python3)\n",
            "frame #42: /usr/bin/python3() [0x5f5506]\n",
            "frame #43: PyRun_FileExFlags + 0x9c (0x5f8c6c in /usr/bin/python3)\n",
            "frame #44: PyRun_SimpleFileExFlags + 0x196 (0x5f9206 in /usr/bin/python3)\n",
            "frame #45: /usr/bin/python3() [0x64faf2]\n",
            "frame #46: _Py_UnixMain + 0x2e (0x64fc4e in /usr/bin/python3)\n",
            "frame #47: __libc_start_main + 0xe7 (0x7fa2c4ec3c87 in /lib/x86_64-linux-gnu/libc.so.6)\n",
            "frame #48: _start + 0x2a (0x5b621a in /usr/bin/python3)\n",
            "\n",
            "/bin/bash: line 1:  2885 Aborted                 (core dumped) ( fairseq-train ../indic-en-exp/final_bin --max-source-positions=210 --max-target-positions=210 --max-update=1000 --save-interval=1 --arch=transformer_4x --criterion=label_smoothed_cross_entropy --source-lang=SRC --lr-scheduler=inverse_sqrt --target-lang=TGT --label-smoothing=0.1 --optimizer adam --adam-betas \"(0.9, 0.98)\" --clip-norm 1.0 --warmup-init-lr 1e-07 --lr 0.0005 --warmup-updates 4000 --dropout 0.2 --save-dir ../indic-en-exp/model --keep-last-epochs 5 --patience 5 --skip-invalid-size-inputs-valid-test --fp16 --user-dir model_configs --update-freq=1 --distributed-world-size 1 --max-tokens 1024 )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "id": "nm_ToBaVRxme"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}